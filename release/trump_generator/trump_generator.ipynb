{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oy9F6GC4wT85"
   },
   "source": [
    "# Character-level RNN for Text Generation\n",
    "\n",
    "This sample code is taken from [Github](https://github.com/jctestud/char-rnn) and demonstrates how to train a stacked LSTM on a set of Tweets and then to generate text that resembles these input Tweets in style. We work with Trump Tweets as collected by the [Trump Twitter Archive](http://www.trumptwitterarchive.com/archive). This form of learning doesn't need manually annotated datasets.\n",
    "\n",
    "\n",
    "![Text Generation Training](https://www.tensorflow.org/text/tutorials/images/text_generation_training.png)\n",
    "\n",
    "\n",
    "## Training\n",
    "The following part demonstrates the training of such an LSTM model. Pay special attention to\n",
    "\n",
    "- how the data batches are generated\n",
    "- what the training data looks like (input and target label)\n",
    "- the model architecture\n",
    "- why do we do this on a character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Woo7v8xlypUa",
    "outputId": "90dfd952-a417-4e68-e7e6-d8480f07e441",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/ADSLab-Salzburg/workshop-chatbot-trump-tweet-generator/main/trump_train.txt\n",
    "!wget -nc https://raw.githubusercontent.com/ADSLab-Salzburg/workshop-chatbot-trump-tweet-generator/main/trump_val.txt\n",
    "!wget -nc https://raw.githubusercontent.com/ADSLab-Salzburg/workshop-chatbot-trump-tweet-generator/main/tweet_preprocessor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukj49AG4CQgH"
   },
   "source": [
    "Some parameters to set and tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SdhpbWqw_Q1"
   },
   "outputs": [],
   "source": [
    "data_directory = \".\"\n",
    "\n",
    "SEQUENCE_LEN = 60\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "HIDDEN_LAYERS_DIM = 512\n",
    "LAYER_COUNT = 4\n",
    "DROPOUT = 0.2\n",
    "\n",
    "MAX_DATA_PERCENT = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuDfKbMJCQgJ"
   },
   "source": [
    "Let's create the vocabulary! Depending on the dataset and your set locale, it might be possible, that you also need to remove other chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIddsXRUCQgM",
    "outputId": "078faa34-f99f-4be5-9385-6e46fbbd5e8a"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "characters = list(string.printable)\n",
    "characters.remove('\\x0b')\n",
    "characters.remove('\\x0c')\n",
    "\n",
    "VOCABULARY_SIZE = len(characters)\n",
    "\n",
    "characters_to_idx = {c:i for i,c in enumerate(characters)}\n",
    "print(f'vocabulary len = {VOCABULARY_SIZE}')\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzZLh9tdCQgO"
   },
   "source": [
    "Here are two helper functions. Just take them as they are. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pl-_ngy0xfbi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_generator(text, count):\n",
    "    \"\"\"Generate batches for training\"\"\"\n",
    "    while True: # keras wants that for reasons\n",
    "        for batch_idx in range(count):\n",
    "            X = np.zeros((BATCH_SIZE, SEQUENCE_LEN, VOCABULARY_SIZE))\n",
    "            y = np.zeros((BATCH_SIZE, VOCABULARY_SIZE))\n",
    "\n",
    "            batch_offset = BATCH_SIZE * batch_idx\n",
    "\n",
    "            for sample_idx in range(BATCH_SIZE):\n",
    "                sample_start = batch_offset + sample_idx\n",
    "                for s in range(SEQUENCE_LEN):\n",
    "                    X[sample_idx, s, characters_to_idx[text[sample_start+s]]] = 1\n",
    "                y[sample_idx, characters_to_idx[text[sample_start+s+1]]]=1\n",
    "\n",
    "            yield X, y\n",
    "\n",
    "\n",
    "def describe_batch(X, y, samples=3):\n",
    "    \"\"\"Describe in a human-readable format some samples from a batch\"\"\"\n",
    "    for i in range(samples):\n",
    "        sentence = \"\"\n",
    "        for s in range(SEQUENCE_LEN):\n",
    "            sentence += characters[X[i,s,:].argmax()]\n",
    "        next_char = characters[y[i,:].argmax()]\n",
    "\n",
    "        print(\"sample #%d: ...%s -> '%s'\" % (\n",
    "            i,\n",
    "            sentence[-20:],\n",
    "            next_char\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0etpn8xCQgS"
   },
   "source": [
    "Your turn!\n",
    "\n",
    "Let's build a model. The model has `LAYER_COUNT` LSTM layers with a Dropout following after each. Each of the LSTMs has the same input shape and the same number of hidden layers. The model has a Dense layer for classification at the end, predicting the characters.\n",
    "Compile it using a categorical cross-entropy and an adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZA98N9hdxgFi"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Build a Keras sequential model for training the char-rnn\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    for layer_idx in range(LAYER_COUNT):\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                HIDDEN_LAYERS_DIM,\n",
    "                return_sequences=True if (layer_idx!=(LAYER_COUNT-1)) else False,\n",
    "                input_shape=(SEQUENCE_LEN, VOCABULARY_SIZE),\n",
    "            )\n",
    "        )\n",
    "        model.add(Dropout(DROPOUT))\n",
    "\n",
    "    model.add(Dense(VOCABULARY_SIZE, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbjJHA7vCQgW"
   },
   "source": [
    "Here, the text is loaded, (its size reduced,) and described initially. How does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axJe7pwmyh64",
    "outputId": "fded02b1-c8d4-404a-c240-da3268e86509"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# loading the text\n",
    "with open(os.path.join(data_directory, \"trump_train.txt\"), \"r\", encoding=\"utf8\") as f:\n",
    "    text_train = f.read()\n",
    "with open(os.path.join(data_directory, \"trump_val.txt\"), \"r\", encoding=\"utf8\") as f:\n",
    "    text_val = f.read()\n",
    "\n",
    "text_train = text_train[:int(MAX_DATA_PERCENT*len(text_train))]\n",
    "text_val = text_val[:int(MAX_DATA_PERCENT*len(text_val))]\n",
    "\n",
    "text_train_len = len(text_train)\n",
    "text_val_len = len(text_val)\n",
    "print(\"Total of %d characters\" % (text_train_len + text_val_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKiTkmd-CQgZ"
   },
   "source": [
    "Let's see, how the `batch_generator` prepares the data. Try out the generator and describe one batch using the `describe_batch(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utFM4Rv4CQgZ",
    "outputId": "89806017-bd3d-4007-d26d-78e2502de0e0"
   },
   "outputs": [],
   "source": [
    "for ix, (X,y) in enumerate(batch_generator(text_train, count=3)):\n",
    "    # describe some samples from the first batch\n",
    "    describe_batch(X, y, samples=5)\n",
    "    if ix > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MaelIFlCQgZ"
   },
   "source": [
    "Let's create a model and have a look at its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uo9u0N4gy0Ic",
    "outputId": "49a6f80f-7085-46ee-8e3e-50d79890136b"
   },
   "outputs": [],
   "source": [
    "training_model = build_model()\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0Dz8CAzCQga"
   },
   "source": [
    "How many batches do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOzfyvZqCQgc",
    "outputId": "9b8d6035-cea2-4195-c17b-171b7e3ac135"
   },
   "outputs": [],
   "source": [
    "train_batch_count = (text_train_len - SEQUENCE_LEN) // BATCH_SIZE\n",
    "val_batch_count = (text_val_len - SEQUENCE_LEN) // BATCH_SIZE\n",
    "print(\"training batch count: %d\" % train_batch_count)\n",
    "print(\"validation batch count: %d\" % val_batch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q787zmRuCQgc"
   },
   "source": [
    "**Callbacks** today we are using standard callbacks. What are both doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBprSkQACQgd"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "filepath = \"./BS-%d_%d-%s_dp%.2f_%dS_epoch{epoch:02d}-loss{loss:.4f}-val-loss{val_loss:.4f}_weights\" % (\n",
    "    BATCH_SIZE,\n",
    "    LAYER_COUNT,\n",
    "    HIDDEN_LAYERS_DIM,\n",
    "    DROPOUT,\n",
    "    SEQUENCE_LEN\n",
    ")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVDArMoMCQge"
   },
   "source": [
    "Let's get the training started!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "vHGMGOFvzChg",
    "outputId": "8b8bf346-e42e-4b0f-c22f-5682acf1a6ce"
   },
   "outputs": [],
   "source": [
    "history = training_model.fit(\n",
    "    batch_generator(text_train, count=train_batch_count),\n",
    "    steps_per_epoch=train_batch_count,\n",
    "    max_queue_size=4,     # no more than one queued batch in RAM\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=batch_generator(text_val, count=val_batch_count),\n",
    "    validation_steps=val_batch_count,\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIZmQ3YqEkYb"
   },
   "source": [
    "### Testing\n",
    "\n",
    "![Text Generation Sampling](https://www.tensorflow.org/text/tutorials/images/text_generation_sampling.png)\n",
    "\n",
    "The following part demonstrates the testing of the model - in other words, how to generate text that sounds like Trump. To keep the prediction step simple, a batch size of 1 is used. In tensorflow v1, the batch size cannot be changed once the model is built. This means that we have to rebuild the model and restore the weights from the training checkpoint.\n",
    "\n",
    "Here, we are building up a new model and loading the weights to the model.\n",
    "\n",
    "Let's start generating text...\n",
    "\n",
    "\n",
    "##### Model\n",
    "In the following, create a model, similar as done in the training section.  This time, you do not need to add dropouts. Additionally, instead of the `input_shape`, you need to define the `batch_input_shape` in such a way, that only on word is passed along.\n",
    "\n",
    "Good think, you don't need to compile the model. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2z1UN14EsfU"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "test_model = Sequential()\n",
    "\n",
    "for i in range(LAYER_COUNT):\n",
    "    test_model.add(\n",
    "            LSTM(\n",
    "                HIDDEN_LAYERS_DIM,\n",
    "                return_sequences=(i!=(LAYER_COUNT-1)),\n",
    "                batch_input_shape=(1, 1, VOCABULARY_SIZE),  # batch size = 1\n",
    "                stateful=True\n",
    "            )\n",
    "        )\n",
    "test_model.add(Dense(VOCABULARY_SIZE, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEdAJ_7hCQgg"
   },
   "source": [
    "Load the weights with the best validation results during the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "4WrElh-5ExXU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b028ad1c9c0afa9066cffd255ff3471",
     "grade": true,
     "grade_id": "cell-29108a94f7cc9cbe",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "533b6847-aca2-4008-a40e-ba5465c5c4ef"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwIGxcT5CQgh"
   },
   "source": [
    "Imagine, a normal setup for a prediction. _Which result is taken from the output distribution?_\n",
    "\n",
    "What can we do in order to have variability in our text generation?\n",
    "\n",
    "Additionaly: the distribution can also be changed. Try to implement a temperature coefficient [1]:\n",
    "$$q_i = \\frac{e^{\\frac{z_i}{T}}}{\\sum_j e^{\\frac{z_j}{T}}}$$ where $T$ can be set to any value in $[0,1]$.\n",
    "\n",
    "For the random sampling, have a look at [numpy.random.multinomial](https://numpy.org/doc/stable/reference/random/generated/numpy.random.multinomial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gGhi9r1CQgh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Helper function to sample an index from a probability array\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)  # ... z\n",
    "\n",
    "    preds = preds / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    # n     ... number of experiments\n",
    "    # pvals ... probability values\n",
    "    # size  ... output shape\n",
    "    probas = np.random.multinomial(n=1, pvals=preds/np.sum(preds), size=1)\n",
    "\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fsMsVSjCQgh"
   },
   "source": [
    "You can consider these two functions as they are.\n",
    "\n",
    "`predict_next_char` predicts a probability distribution for the following character, based on an input. This probability distribution is then used to sample characters based on this distribution.\n",
    "\n",
    "The `generate_text`functions generates a text using a model, based on the `seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIxTH9ecEzKz"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def predict_next_char(model, current_char, diversity=1.0):\n",
    "    \"\"\"Predict the next character from the current one\"\"\"\n",
    "    x = np.zeros((1, 1, VOCABULARY_SIZE))\n",
    "    x[:,:,characters_to_idx[current_char]] = 1\n",
    "    y = model.predict(x, batch_size=1,verbose=0,)\n",
    "    next_char_idx = sample(y[0,:], temperature=diversity)\n",
    "    next_char = characters[next_char_idx]\n",
    "    return next_char\n",
    "\n",
    "def generate_text(model, seed=\"I am\", count=140):\n",
    "    \"\"\"Generate characters from a given seed\"\"\"\n",
    "    model.reset_states()\n",
    "    for s in seed[:-1]:\n",
    "        next_char = predict_next_char(model, s)\n",
    "    current_char = seed[-1]\n",
    "\n",
    "    sys.stdout.write(\"[\"+seed+\"]\")\n",
    "\n",
    "    # no more reset, preserve context\n",
    "    for i in range(count - len(seed)):\n",
    "        next_char = predict_next_char(model, current_char, diversity=0.7)\n",
    "        current_char = next_char\n",
    "        sys.stdout.write(next_char)\n",
    "    print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbQsg-Y5CQgi"
   },
   "source": [
    "Now, let's write some tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqJl8Q_1E1rH",
    "outputId": "a61d5a31-93ba-4cba-ede9-0711ea2b641f"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    generate_text(\n",
    "        test_model,\n",
    "        seed=\"Despite the constant negative press \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjmwObF2CQgj"
   },
   "source": [
    "### Try it with your seed sentence.\n",
    "\n",
    "The question that unanswered till now: what did he want to tell us?\n",
    "\n",
    "[![covefe](https://pbs.twimg.com/media/DBIXi67V0AAzS7x?format=jpg&name=900x900)](https://twitter.com/GavinNewsom/status/869783572390883328/photo/1)\n",
    "\n",
    "# Homework\n",
    "Retry this with your dataset. You can use, whatever you want! Linux-Kernel or other source code, novels, or even your former assignments from school to create a personalized assignment generator!\n",
    "\n",
    "# Literature\n",
    "[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015). [arXiv](https://arxiv.org/abs/1503.02531)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "dl_lm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
